<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
 <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>RabbitMQ Performance</title>
    <link href="perf.css" rel="stylesheet" type="text/css">
    <!--[if lte IE 8]><script language="javascript" type="text/javascript" src="lib/excanvas.min.js"></script><![endif]-->
    <script language="javascript" type="text/javascript" src="lib/jquery.min.js"></script>
    <script language="javascript" type="text/javascript" src="lib/jquery.flot.min.js"></script>
    <script language="javascript" type="text/javascript" src="perf.js"></script>
 </head>
    <body>
    <h1>RabbitMQ Performance</h1>

    <p>
      So today I would like to talk about some aspects of RabbitMQ's
      performance. There are a huge number of variables that feed into
      the overall level of performance you can get from a RabbitMQ
      server, and today we're going to try tweaking some of them and
      seeing what we can see.
    </p>

    <p>
      The aim of this piece is not to try to convince you that
      RabbitMQ is the fastest message broker in the world - it often
      isn't (although we like to think we're still pretty decent) -
      but to give you some ideas about what sort of performance you
      can expect in different situations.
    </p>

    <p>
      All the charts and statistics shown were measured on a PowerEdge
      R610 with dual Xeon E5530s and 40GB RAM. Largely because it was
      the fastest machine we had lying around. One major thing that's
      not ideal is we ran the clients on the same machine as the
      server - just due to the limited hardware we had available.
    </p>

    <p>
      By the way, the code to produce all these statistics
      is available in branch bug24527 of rabbitmq-java-client
      (although it's currently rather rough) Eventually it will get
      merged to default, and also become easier to work with. We hope.
    </p>

    <h2>Flow control in RabbitMQ 2.8.0</h2>

    <p>
      But first of all I need to introduce a new feature in RabbitMQ
      2.8.0+ - internal flow control. RabbitMQ is internally made up of
      a number of Erlang processes which pass messages to each
      other. Each process has a <em>mailbox</em> which contains
      messages it has recieved and not yet handled. And these
      mailboxes can grow to an unbounded size.
    </p>

    <p>
      What this means is that unless the first process to receive data
      off a network socket is the slowest in the chain, (it's not)
      then when you have a heavily-loaded RabbitMQ server messages can
      build up in process mailboxes forever. Or rather, until we run
      out of memory. Or rather, until the memory alarm goes off. At
      which point the server will stop accepting new messages while it
      sorts itself out.
    </p>

    <p>
      The trouble is, this can take some time. The following chart
      (the only one in this post made against RabbitMQ 2.7.1) shows a
      simple process that publishes small messages into the broker as
      fast as possible, and also consumes them as fast as possible,
      with acknowledgement, confirms, persistence and so on all
      switched off. We plot the sending rate, the receiving rate, and
      the latency (time taken for a sent message to be received), over
      time. Note that the latency is a logarithmic scale.
    </p>

    <h3>Simple 1 -> 1 autoack (2.7.1)</h3>
    <div class="chart"
         data-type="time"
         data-latency="true"
         data-x-axis="time (s)"
         data-y-axis="rate (msg/s)"
         data-y-axis2="latency (μs)"
         data-file="results-mini-2.7.1.js"
         data-scenario="no-ack-long"></div>

    <p>
      Ouch! That's rather unpleasant. Several things should be obvious:
    </p>

    <ul>
      <li>The send rate and receive rate fluctuate quite a lot.</li>
      <li>The send rate drops to zero for two minutes (this is the
      first time the memory alarm went off). In fact the memory alarm
      goes off again at the end.</li>
      <li>The latency increases steadily (and look at the scale - we
      show microseconds, but we could just as easily measure it in
      minutes).</li>
    </ul>

    <p>
      (The small drop in latency around 440s is due to all the
      messages published before 200s being consumed, and the long gap
      afterwards.)
    </p>

    <p>
      Of course, this is only the sort of behaviour you would expect
      when stressing a server to the limit. But we're benchmarking -
      we want to do that. And anyway, servers get stressed in
      production too.
    </p>

    <p>
      So now let's look at the same experiment conducted against a
      RabbitMQ 2.8.1 server:
    </p>

    <h3>Simple 1 -> 1 autoack (2.8.1)</h3>
    <div class="chart"
         data-type="time"
         data-latency="true"
         data-x-axis="time (s)"
         data-y-axis="rate (msg/s)"
         data-y-axis2="latency (μs)"
         data-scenario="no-ack-long"></div>

    <p>
      That looks like a much calmer experience! The send rate, receive
      rate and latency are all near-constant. The reason is internal
      flow control. The latency is around 400ms (which is still quite
      high compared to a less loaded server for reasons I'll discuss
      in a minute).
    </p>

    <p>
      These charts don't show memory consumption, but the story is the
      same - in this circumstance 2.7.1 will eat lots of memory and
      bounce off the memory alarm threshold, and 2.8.1 will use a
      fairly constant, fairly low quantity of memory.
    </p>

    <p>
      Each process in the chain issues <em>credit</em> to the
      processes that can send messages to it. Processes consume credit
      as they send messages, and issue more credit as they receive
      them. When a process runs out of credit it will stop issuing
      more to its upstream processes. Eventually we reach the process
      which is reading bytes off a network
      socket. When <strong>that</strong> process runs out of credit,
      it stops reading until it gets more. This is the same as when
      the memory alarm goes off for the 2.7.1 broker, except that it
      happens many times per second rather than taking minutes, and we
      control memory use a lot more.
    </p>

    <p>
      So where does that 400ms latency come from? Well, there are
      still messages queueing up at each stage in the pipeline, so it
      takes a while for a message to get from the beginning to the
      end. That accounts for some of the latency. However, most of it
      comes from an invisible "mailbox" in front of the entire server
      - the TCP buffers provided by the operating system. On Linux
      the OS will allow up to 8MB of messages to back up in the TCP
      stack. 8MB doesn't sound like a lot of course, but we're dealing
      with tiny messages (and each one needs routing decisions,
      permissions check and so on to be made).
    </p>

    <p>
      But it's important to remember that we tend to see the worst
      latency when running at the limit of what we can do. So here's
      one final chart for this week:
    </p>

    <h3>1 -> 1 sending rate attempted vs latency</h3>
    <div class="chart"
         data-type="r-l"
         data-x-axis="rate attempted (msg/s)"
         data-y-axis="rate (msg/s)"
         data-scenario="rate-vs-latency"></div>

    <p>
      Note that the horizontal axis is no longer time. We're now
      showing the results of many runs like the ones above, with each
      point representing one run.
    </p>

    <p>
      In the charts above we were running as fast as we can, but here
      we limit the rate at varying points up to the maximum rate we
      can achieve. So the yellow line shows rate attempted vs rate
      achieved - see that it goes most of the way purely 1:1 linearly
      (when we have spare capacity and so if we try to publish faster
      we will succeed) and then stops growing as we reach the limit of
      what we can do.
    </p>

    <p>
      But look at the latency! With low publishing rates we have
      latency of considerably less than a millisecond. But this drifts
      up as the server gets busier. As we stop being able to publish
      any faster, we hit a wall of latency - the TCP buffers start to
      fill up and soon messages are taking hundreds of milliseconds to
      get through them.
    </p>

    <p>
      So hopefully we've shown how RabbitMQ 2.8.1 offers much more
      reliable performance when heavily loaded than previous versions,
      and shown how latency can reach for the skies when your message
      broker is overloaded. Tune in next time to see how some
      different ways of using messaging affect performance!
    </p>

    <hr/>
    <p>
      Welcome back! Last time we talked about flow control and
      latency; today let's talk about how different features affect
      the performance we see. Here are some simple scenarios. As
      before, they're all variations on the theme of one publisher and
      one consumer publishing as fast as they can.
    </p>

    <h2>Some Simple Scenarios</h2>

    <div class="box">
      <div class="summary" data-scenario="no-ack">auto-ack</div>
      <div class="small-chart"
           data-type="time"
           data-x-axis="time (s)"
           data-y-axis=""
           data-scenario="no-ack"></div>
      <p>
        This first scenario is the simplest - just one producer and
        one consumer. So we have a baseline.
      </p>
    </div>

    <div class="box">
      <div class="summary" data-scenario="no-consume">no-consume</div>
      <div class="small-chart"
           data-type="time"
           data-x-axis="time (s)"
           data-y-axis=""
           data-scenario="no-consume"></div>
      <p>
        Of course we want to produce impressive figures. So we can go
        a bit faster than that - if we don't consume anything then we
        can publish faster.
      </p>
    </div>

    <div class="box">
      <div class="summary" data-scenario="headline-publish" mode="send">max publish</div>
      <div class="small-chart"
           data-type="time"
           data-x-axis="time (s)"
           data-y-axis=""
           data-scenario="headline-publish"></div>
      <p>
        This uses a couple of the cores on our server - but not all of
        them. So for the best headline-grabbing rate, we start a
        number of parallel producers, all publishing into nothing.
      </p>
    </div>

    <div class="box">
      <div class="summary" data-scenario="headline-consume" mode="recv">max consume</div>
      <div class="small-chart"
          data-type="time"
           data-x-axis="time (s)"
           data-y-axis=""
           data-scenario="headline-consume"></div>
      <p>
        Of course, consuming is rather important! So for the headline
        consuming rate, we publish to a large number of consumers in
        parallel.
      </p>
    </div>

    <p>
      Of course to some extent this quest for large numbers is a bit
      silly, we're more interested in relative performance. So let's
      revert to one producer and one consumer.
    </p>

    <div class="box">
      <div class="summary" data-scenario="no-ack-mandatory">mandatory</div>
      <div class="small-chart" 
           data-type="time" 
           data-x-axis="time (s)" 
           data-y-axis=""
           data-scenario="no-ack-mandatory"></div>
      <p>
        Now let's try publishing with the mandatory flag set. We drop
        to about 40% of the non-mandatory rate. The reason for this is
        that the channel we're publishing to can't just asynchronously
        stream messages at queues any more; it synchronously checks
        with the queues to make sure they're still there. (Yes, we
        could probably make mandatory publishing faster, but it's not
        very heavily used.)
      </p>
    </div>

    <div class="box">
      <div class="summary" data-scenario="no-ack-immediate">immediate</div>
      <div class="small-chart" 
           data-type="time" 
           data-x-axis="time (s)" 
           data-y-axis=""
           data-scenario="no-ack-immediate"></div>
      <p>
        The immediate flag gives us almost exactly the same drop in
        performance. This isn't hugely surprising - it has to make the
        same synchronous check with the queue.
      </p>
    </div>

    <div class="box">
      <div class="summary" data-scenario="ack">ack</div>
      <div class="small-chart" 
           data-type="time" 
           data-x-axis="time (s)" 
           data-y-axis=""
           data-scenario="ack"></div>
      <p>
        Scrapping the rarely-used mandatory and immediate flags, let's
        try turning on acknowledgements for delivered messages. We still
        see a performance drop compared to delivering without
        acknowledgements (the server has to do more bookkeeping after
        all) but it's less noticeable.
      </p>
    </div>

    <div class="box">
      <div class="summary" data-scenario="ack-confirm">ack-confirm</div>
      <div class="small-chart" 
           data-type="time" 
           data-x-axis="time (s)" 
           data-y-axis=""
           data-scenario="ack-confirm"></div>
      <p>
        Now we turn on publish confirms as well. Performance drops a
        little more but we're still at over 60% the speed of neither
        acks nor confirms.
      </p>
    </div>

    <div class="box">
      <div class="summary"
           data-scenario="ack-confirm-persist">a-c-persist</div>
      <div class="small-chart" 
           data-type="time" 
           data-x-axis="time (s)" 
           data-y-axis=""
           data-scenario="ack-confirm-persist"></div>
      <p>
        Finally, we enable message persistence. The rate becomes much
        lower, since we're throwing all those messages at the disk as
        well.
      </p>
    </div>

    <h2>Message Sizes</h2>

    <p>
      Notably, all the messages we've been sending until now have only
      been a few bytes long. There are a couple of reasons for this:
    </p>

    <ul>
      <li>Quite a lot of the work done by RabbitMQ is per-message, not
      per-byte-of-message.</li>
      <li>It's always nice to look at big numbers.</li>
    </ul>

    <p>
      But in the real world we will often want to send bigger
      messages. So let's look at the next chart:
    </p>

    <h3>1 -> 1 sending rate message sizes</h3>
    <div class="chart"
         data-type="x-y"
         data-scenario="message-sizes-large"
         data-x-key="minMsgSize"
         data-plot-keys="send-msg-rate send-bytes-rate"
         data-x-axis="message size (bytes)"
         data-y-axis="rate (msg/s)"
         data-y-axis2="rate (bytes/s)"
         data-legend="ne"></div>

    <p>
      Here (again) we're sending unacked / unconfirmed messages as
      fast as possible, but this time we vary the message size. We
      can see that (of course) the message rate drops further as the
      size increases, but the actual number of bytes sent increases as
      we have less and less routing overhead.
    </p>

    <p>
      So how does the message size affect horizontal scaling? Let's
      vary the number of producers with different message sizes. Just
      for a change, in this test we're not going to have any consumers
      ar all.
    </p>

    <h3>n -> 0 sending msg rate vs number of producers, for various message sizes</h3>
    <div class="chart"
         data-type="series"
         data-scenario="message-sizes-and-producers"
         data-x-key="producerCount"
         data-x-axis="producers"
         data-y-axis="rate (msg/s)"
         data-plot-key="send-msg-rate"
         data-series-key="minMsgSize"></div>

    <h3>n -> 0 sending bytes rate vs number of producers, for various message sizes</h3>
    <div class="chart"
         data-type="series"
         data-scenario="message-sizes-and-producers"
         data-x-key="producerCount"
         data-x-axis="producers"
         data-y-axis="rate (bytes/s)"
         data-plot-key="send-bytes-rate"
         data-series-key="minMsgSize"></div>

    <p>
      In these tests we can see that for small messages it only takes
      a couple of producers to reach an upper bound on how many
      messages we can publish, but that for larger messages we need
      more producers to use the available bandwidth.
    </p>

    <p>
      Another frequently confusing issue is performance around
      consumers with a prefetch count. RabbitMQ (well, AMQP) defaults
      to sending all the messages it can to any consumer that looks
      ready to accept them. The maximum number of these unacknowledged
      messages per channel can be limited by setting the prefetch
      count. However, small prefetch counts can hurt performance
      (since we can be waiting for acks to arrive before sending out
      more messages).
    </p>

    <p>
      So let's have a look at prefetch count and, while we're there,
      also consider the number of consumers consuming from a single
      queue. This chart contains some deliberately absurd extremes.
    </p>

    <h3>1 -> n recving rate vs consumer count / prefetch count</h3>
    <div class="chart"
         data-type="series"
         data-scenario="consumers"
         data-x-key="consumerCount"
         data-x-axis="number of consumers"
         data-x-axis-log="true"
         data-y-axis="rate (msg/s)"
         data-legend="ne"
         data-plot-key="recv-msg-rate"
         data-series-key="prefetchCount"></div>

    <p>
      The first thing to notice is that tiny prefetch counts really
      hurt performance. Note the large difference in performance
      between prefetch = 1 and prefetch = 2! But we also get into
      diminishing returns - notice that the difference between
      prefetch = 20 and prefetch = 50 is hard to see, and the
      difference between prefetch = 50 and prefetch = 10000 is almost
      invisible. Of course, this is because for our particular network
      link prefetch = 50 already ensures that we never starve the
      consumer while waiting for acks. Of course, this test was run
      over a low latency link - more latent links will benefit from a
      higher prefetch count.
    </p>

    <p>
      The second thing to notice is that when we have a small number
      of consumers, adding one more will increase performance (we get
      more parallellism). And with a tiny prefetch count, increasing
      consumers even up to a large number has benefits (since each
      individual consumer spends much of its time starved). But when
      we have a larger prefetch count, increasing the number of
      consumers is not so helpful, since even a small number can kept
      busy enough to max out our queue, but the more consumers we have
      the more work RabbitMQ has to do to keep track of all of them.
    </p>

    <h2>Large queues</h2>

    <p>
      All the examples we've looked at so far have one thing in
      common: very few messages actually get queued. In general we've
      looked at scenarios where messages get consumed as quickly as
      they get produced, and thus each queue has an average length of
      0.
    </p>

    <p>
      So what happens whe queues get big? When queues are small(ish)
      they will reside entirely within memory. Persistent messages
      will also get written to disc, but they will only get read again
      if the broker restarts.
    </p>

    <p>
      But when queues get larger, they will get paged to disc,
      persistent or not. In this case performance can take a hit as
      suddenly we need to access the disc to send messages to
      consumers. So let's run a test: publish a lot of non-persistent
      messages to a queue, and then consume them all.
    </p>

    <h3>Queue load / drain 500k messages</h3>
    <div class="chart"
         data-type="time"
         data-x-axis="time (s)"
         data-y-axis="rate (msg/s)"
         data-scenario="fill-drain-small-queue"></div>

    <p>
      In this small case we can see fairly consistent performance:
      the messages go into the queue fairly quickly and then come out
      even more quickly.
    </p>

    <h3>Queue load / drain 5M messages</h3>
    <div class="chart"
         data-type="time"
         data-x-axis="time (s)"
         data-y-axis="rate (msg/s)"
         data-scenario="fill-drain-large-queue"></div>

    <p>
      But when we have a larger queue we see that the performance
      varies a lot more. We see that when loading the queue we
      initially get a very high throughput, then a pause while some of
      the queue is paged out to disc, then a more consistent lower
      throughput. Similarly when draining the queue we see a much
      lower rate when pulling the messages from disc.
    </p>

    <p>
      Performance of disc-bound queues is a complex topic -
      see <a href="http://www.rabbitmq.com/blog/2011/10/27/performance-of-queues-when-less-is-more/">Matthew's
      blog post on the subject</a> for some more talk on the subject.
      </p>
 </body>
</html>
